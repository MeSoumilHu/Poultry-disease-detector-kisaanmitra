{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da68127-a73f-4210-9115-2ab61be18598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# open file\n",
    "from IPython.display import Image\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "# model CNN (Deep learning network)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense,\\\n",
    "GlobalAveragePooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba78154-30d5-4d04-98e2-54958367bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function plot loss function and accuracy score graph\n",
    "def plot_graph(model_values):\n",
    "    ''' \n",
    "    Input : Model_values of keras.callbacks.History\n",
    "    Return : Graph of Loss function and accuracy score between training dataset and vaildation dataset\n",
    "    '''\n",
    "    # Subplots\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(model_values.history['loss'], label='Training Loss');\n",
    "    plt.plot(model_values.history['val_loss'], label='Testing Loss');\n",
    "    plt.legend(fontsize=12, loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss');\n",
    "    \n",
    "    # Plot MSE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    plt.plot(model_values.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(model_values.history['val_accuracy'], label='Validation Accuracy')\n",
    "    \n",
    "    plt.legend(fontsize=12, loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6978793-6358-467e-869c-b77c40c2002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6985485098360419511\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e0e67b-08a3-449e-824b-c04586186e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to detect a GPU.\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "  print(\"Found a GPU with the name:\", gpu)\n",
    "else:\n",
    "  print(\"Failed to detect a GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee26f98b-4848-42cc-8a58-e3f1adb7d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open path file of dataset \n",
    "# We prepare actucl diretories form to training in 01.Preprocessing datasets notebook\n",
    "dataset_path_new = \"dataset_train_valid_test\"\n",
    "\n",
    "train_dir = os.path.join(dataset_path_new, \"train\")\n",
    "valid_dir = os.path.join(dataset_path_new, \"valid\")\n",
    "test_dir = os.path.join(dataset_path_new, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84c0e9e1-9b9a-45bd-81c0-6ae4658d1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter of image \n",
    "# We use input small size image because reduce size of model \n",
    "batch_size = 50 # Set the batch size for epoch cycle\n",
    "img_height = 128 # Set the height of the picture\n",
    "img_width = 128 # Set the width of the picture\n",
    "\n",
    "# Rescale pixel to reduce image size before using in model\n",
    "data_gen_train = ImageDataGenerator(rescale=1/255.)\n",
    "data_gen_valid = ImageDataGenerator(rescale=1/255.)\n",
    "data_gen_test = ImageDataGenerator(rescale=1/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb17235b-4c99-4a5f-b57f-4bebb2ea9cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6367 images belonging to 4 classes.\n",
      "Found 686 images belonging to 4 classes.\n",
      "Found 668 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset \n",
    "train_dataset = data_gen_train.flow_from_directory(train_dir,\n",
    "                                                   class_mode=\"categorical\",\n",
    "                                                   target_size=(img_height, img_width),\n",
    "                                                   batch_size=batch_size)\n",
    " \n",
    "\n",
    "# Create validation dataset \n",
    "valid_dataset = data_gen_valid.flow_from_directory(valid_dir,\n",
    "                                                   class_mode=\"categorical\",\n",
    "                                                   target_size=(img_height, img_width),\n",
    "                                                   batch_size=batch_size)\n",
    "\n",
    "# Create testing dataset \n",
    "test_dataset = data_gen_test.flow_from_directory(test_dir,\n",
    "                                                   class_mode=\"categorical\",\n",
    "                                                   target_size=(img_height, img_width),\n",
    "                                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3caac3cd-965f-493f-b8ac-62e0b906d10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    28.647715\n",
      "0    26.825821\n",
      "1    25.490812\n",
      "2    19.035653\n",
      "Name: proportion, dtype: float64\n",
      "{'cocci': 0, 'healthy': 1, 'ncd': 2, 'salmo': 3}\n"
     ]
    }
   ],
   "source": [
    "# Classes in training dataset\n",
    "print(pd.Series(train_dataset.classes).value_counts(normalize = True).mul(100))\n",
    "print(train_dataset.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40af858e-278c-4b6e-a323-f9bd29a7918b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    240\n",
       "3    207\n",
       "0    199\n",
       "2     40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classes in validation dataset\n",
    "pd.Series(valid_dataset.classes).value_counts()\n",
    "# Class 0 : cocci\n",
    "# Class 1 : healthy\n",
    "# Class 2 : ncd \n",
    "# Class 3 : salmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0129c462-b78b-49b0-b409-a22f3975c01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    245\n",
       "0    196\n",
       "1    194\n",
       "2     33\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classes in training dataset\n",
    "pd.Series(test_dataset.classes).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f784e0dc-7675-4ccc-8ed8-9c1f8595ec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92549026\n",
      "0.039215688\n"
     ]
    }
   ],
   "source": [
    "images, labels_class = next(train_dataset)\n",
    "classes = train_dataset.class_indices\n",
    "print(images[0].max())\n",
    "print(images[0].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b09dadf1-51a9-4022-8d76-d3dad202e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "# Compile the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Conv2D(16,(3,3), activation='relu',\n",
    "                 kernel_initializer='he_uniform', \n",
    "                 padding='same', input_shape=(128,128,3))) # filter image by dot product in matrix to find the object in picture\n",
    "# Hidden layers\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) # reduce stucture of image even if object slightly move but the result is same  \n",
    "model.add(Conv2D(32,(3,3), activation='relu',\n",
    "                 kernel_initializer='he_uniform', \n",
    "                 padding='same')) \n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten()) # convert all the resultant 2-Dimensional arrays into a single long continuous linear vector\n",
    "model.add(Dense(64, activation='relu',\n",
    "                kernel_initializer='he_uniform'))\n",
    "\n",
    "# avoid overfitting!\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model         \n",
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cd25cf2-6392-4a83-819b-a50170d786e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 128, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 64, 64, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 32, 32, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32768)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2097216   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,102,564\n",
      "Trainable params: 2,102,564\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# we have 2,102,564 nodes for training\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b01207c9-c502-4a1a-8fc1-f490cf0209aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoints during training\n",
    "checkpoint_path = \"../model/cnn/cnn_model_cp/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f27273b0-7e61-4541-9d53-ce2bb1946ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.8384 - accuracy: 0.3471\n",
      "Epoch 1: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 430s 3s/step - loss: 1.8384 - accuracy: 0.3471 - val_loss: 0.9772 - val_accuracy: 0.5262\n",
      "Epoch 2/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.1140 - accuracy: 0.4450\n",
      "Epoch 2: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 415s 3s/step - loss: 1.1140 - accuracy: 0.4450 - val_loss: 0.8745 - val_accuracy: 0.5481\n",
      "Epoch 3/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 1.0632 - accuracy: 0.4475\n",
      "Epoch 3: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 418s 3s/step - loss: 1.0632 - accuracy: 0.4475 - val_loss: 0.7968 - val_accuracy: 0.5816\n",
      "Epoch 4/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.9950 - accuracy: 0.5048\n",
      "Epoch 4: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 411s 3s/step - loss: 0.9950 - accuracy: 0.5048 - val_loss: 0.7464 - val_accuracy: 0.7799\n",
      "Epoch 5/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.9377 - accuracy: 0.5748\n",
      "Epoch 5: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 412s 3s/step - loss: 0.9377 - accuracy: 0.5748 - val_loss: 0.6979 - val_accuracy: 0.8367\n",
      "Epoch 6/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.8524 - accuracy: 0.6576\n",
      "Epoch 6: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 383s 3s/step - loss: 0.8524 - accuracy: 0.6576 - val_loss: 0.6522 - val_accuracy: 0.8411\n",
      "Epoch 7/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.7540 - accuracy: 0.7030\n",
      "Epoch 7: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 383s 3s/step - loss: 0.7540 - accuracy: 0.7030 - val_loss: 0.5519 - val_accuracy: 0.8440\n",
      "Epoch 8/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.6708 - accuracy: 0.7382\n",
      "Epoch 8: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 389s 3s/step - loss: 0.6708 - accuracy: 0.7382 - val_loss: 0.5541 - val_accuracy: 0.8630\n",
      "Epoch 9/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.6286 - accuracy: 0.7767\n",
      "Epoch 9: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 404s 3s/step - loss: 0.6286 - accuracy: 0.7767 - val_loss: 0.4995 - val_accuracy: 0.9009\n",
      "Epoch 10/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.5863 - accuracy: 0.7897\n",
      "Epoch 10: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 387s 3s/step - loss: 0.5863 - accuracy: 0.7897 - val_loss: 0.4830 - val_accuracy: 0.8776\n",
      "Epoch 11/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.5318 - accuracy: 0.8029\n",
      "Epoch 11: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 407s 3s/step - loss: 0.5318 - accuracy: 0.8029 - val_loss: 0.3999 - val_accuracy: 0.8950\n",
      "Epoch 12/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.4857 - accuracy: 0.8175\n",
      "Epoch 12: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 391s 3s/step - loss: 0.4857 - accuracy: 0.8175 - val_loss: 0.3490 - val_accuracy: 0.9082\n",
      "Epoch 13/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.8319\n",
      "Epoch 13: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 379s 3s/step - loss: 0.4440 - accuracy: 0.8319 - val_loss: 0.3345 - val_accuracy: 0.9067\n",
      "Epoch 14/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.4326 - accuracy: 0.8327\n",
      "Epoch 14: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 384s 3s/step - loss: 0.4326 - accuracy: 0.8327 - val_loss: 0.3638 - val_accuracy: 0.9155\n",
      "Epoch 15/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.4154 - accuracy: 0.8418\n",
      "Epoch 15: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 429s 3s/step - loss: 0.4154 - accuracy: 0.8418 - val_loss: 0.3276 - val_accuracy: 0.9155\n",
      "Epoch 16/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.3712 - accuracy: 0.8638\n",
      "Epoch 16: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 414s 3s/step - loss: 0.3712 - accuracy: 0.8638 - val_loss: 0.3556 - val_accuracy: 0.9125\n",
      "Epoch 17/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.3532 - accuracy: 0.8734\n",
      "Epoch 17: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 414s 3s/step - loss: 0.3532 - accuracy: 0.8734 - val_loss: 0.3110 - val_accuracy: 0.9140\n",
      "Epoch 18/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.3424 - accuracy: 0.8731\n",
      "Epoch 18: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 402s 3s/step - loss: 0.3424 - accuracy: 0.8731 - val_loss: 0.3505 - val_accuracy: 0.9169\n",
      "Epoch 19/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.3501 - accuracy: 0.8695\n",
      "Epoch 19: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 392s 3s/step - loss: 0.3501 - accuracy: 0.8695 - val_loss: 0.3383 - val_accuracy: 0.9169\n",
      "Epoch 20/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.3170 - accuracy: 0.8795\n",
      "Epoch 20: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 389s 3s/step - loss: 0.3170 - accuracy: 0.8795 - val_loss: 0.3439 - val_accuracy: 0.9052\n",
      "Epoch 21/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.8874\n",
      "Epoch 21: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 391s 3s/step - loss: 0.2988 - accuracy: 0.8874 - val_loss: 0.3343 - val_accuracy: 0.9023\n",
      "Epoch 22/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.3172 - accuracy: 0.8833\n",
      "Epoch 22: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 395s 3s/step - loss: 0.3172 - accuracy: 0.8833 - val_loss: 0.2629 - val_accuracy: 0.9271\n",
      "Epoch 23/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.9020\n",
      "Epoch 23: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 390s 3s/step - loss: 0.2671 - accuracy: 0.9020 - val_loss: 0.2954 - val_accuracy: 0.9227\n",
      "Epoch 24/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.8996\n",
      "Epoch 24: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 390s 3s/step - loss: 0.2736 - accuracy: 0.8996 - val_loss: 0.2826 - val_accuracy: 0.9286\n",
      "Epoch 25/25\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.9133\n",
      "Epoch 25: saving model to ../model/cnn/cnn_model_cp\\cp.ckpt\n",
      "128/128 [==============================] - 394s 3s/step - loss: 0.2387 - accuracy: 0.9133 - val_loss: 0.3053 - val_accuracy: 0.9169\n"
     ]
    }
   ],
   "source": [
    "# training model \n",
    "# make sure you truely save checkpoint_path\n",
    "history = model.fit(train_dataset,\n",
    "                              epochs=25,\n",
    "                              validation_data=valid_dataset,\n",
    "                              callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7424f4ad-cdc6-4523-84a4-ef4ce480e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/cnn/cnn_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8f61dbd-9ae4-4674-a4ac-e781fa2229ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.838431</td>\n",
       "      <td>0.347102</td>\n",
       "      <td>0.977244</td>\n",
       "      <td>0.526239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.113976</td>\n",
       "      <td>0.444951</td>\n",
       "      <td>0.874544</td>\n",
       "      <td>0.548105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.063180</td>\n",
       "      <td>0.447463</td>\n",
       "      <td>0.796834</td>\n",
       "      <td>0.581633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.994971</td>\n",
       "      <td>0.504790</td>\n",
       "      <td>0.746442</td>\n",
       "      <td>0.779883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.937736</td>\n",
       "      <td>0.574839</td>\n",
       "      <td>0.697862</td>\n",
       "      <td>0.836735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  1.838431  0.347102  0.977244      0.526239\n",
       "1  1.113976  0.444951  0.874544      0.548105\n",
       "2  1.063180  0.447463  0.796834      0.581633\n",
       "3  0.994971  0.504790  0.746442      0.779883\n",
       "4  0.937736  0.574839  0.697862      0.836735"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_df = pd.DataFrame(history.history) \n",
    "hist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71264afa-0821-4923-8020-25c173cd4a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save history our model to csv: \n",
    "hist_csv_file = '../model/cnn/history_cnn16.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f6cc9-1f23-4ea6-8d9c-5c6249fd7fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
